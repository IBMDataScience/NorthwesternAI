{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#1d3649; height:100px; color:white; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;font-weight: bold;\">Want to do more?</span><span style=\"border: 1px solid #41d6c3;padding: 15px;float:right;margin-right:40px; color:#41d6c3; \">\n",
    "<a href=\" https://ibm.co/Ndsxreg\" target=\"_blank\" style=\"color: #41d6c3;text-decoration: none;\">Sign Up</a></span><br>\n",
    "<span> Try out this notebook with your free trial of Data Science Experience</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Develop a Scala Spark Model on Chicago Building Violations\n",
    "\n",
    "\n",
    "<!--[comment]:This notebook is the first of a series of three notebooks which combine the strengths of Data Science Experience (DSX) Cloud and DSX Local to model building violations in Chicago.\n",
    "The notebooks in this series are:-->\n",
    "\n",
    "<!--1. This notebook, runs on Scala on DSX Cloud\n",
    "2. *Score data with R4WML and R*, runs on R on DSX Local\n",
    "3. *Spam-N-Bayes*, runs on Python on DSX Local-->   \n",
    "\n",
    "This notebook models building violations in Chicago.\n",
    "___________\n",
    "\n",
    "The data are <a href=\"https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr\"  target=\"_blank\" rel=\"noopener noreferrer\">Violations issued by the Chicago Department of Buildings</a>\n",
    " over the period from 2006 until present. The dataset contains instances of `violations`. Each violation is associated with an `inspection` and an `inspection status`. \n",
    "\n",
    "Using Spark Machine Learning, we're going to develop a model for the data from 2006-2016 which provides a score in the interval $[0,1]$ for how likely we believe an individual building is to `Pass` or `Fail` an inspection. \n",
    "\n",
    "This notebook can be used as a companion to another <a href=\"https://medium.com/@adammassachi/dsx-hybrid-mode-91b580450c5b\"  target=\"_blank\" rel=\"noopener noreferrer\">tutorial on our blog</a>. \n",
    "\n",
    "This notebook runs on Scala 2.11 with Spark 2.0.\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Table of contents\n",
    "1. [Wrangle data](#wrangle)\n",
    "2. [Build a pipeline](#build)\n",
    "3. [Train the model](#train)\n",
    "4. [Save the model](#save)\n",
    "5. [Deploy the model](#deploy)\n",
    "6. [Next steps](#summary)\n",
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"wrangle\"></a>\n",
    "## 1. Wrangle data \n",
    "\n",
    "Read the data into a Spark DataFrame. \n",
    "\n",
    "1. [Import the libraries](#libraries)\n",
    "2. [Load the credentials](#credentials)\n",
    "3. [Load the data set](#dataset)\n",
    "4. [Modify the data](#modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Import the libraries<a id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import top level\n",
    "import scala.sys.process._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.util._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.expressions.DateFormatClass\n",
    "import com.ibm.ibmos2spark.bluemix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 Load the credentials<a id='credentials'></a>\n",
    "We need our credentials to work with object storage to read the data into a Spark Dataframe. \n",
    "To load our credentials and the data set:\n",
    "1. Go to the <a href=\"https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr\" target=\"_blank\" rel=\"noopener noreferrer\">Violations issued by the Chicago Department of Buildings</a>. \n",
    "2. Click export, then download and save the data set `Building_Violations` as a.csv file to your computer.  \n",
    "3. Load the .csv file into your notebook. Click the **Data** icon on the notebook action bar. Drop the file into the box or browse to select the file. The file is loaded to your object storage and appears in the Data Assets section of the project. For more information, see <a href=\"https://datascience.ibm.com/docs/content/analyze-data/load-and-access-data.html\" target=\"_blank\" rel=\"noopener noreferrer\">Load and access data</a>.\n",
    "4. Load the credentials for the `Building_Violations.csv` file. Click in the next code cell and select **Insert to code > Insert credentials**.\n",
    "6. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// @hidden_cell\n",
    "// Put your credentials here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load the data set<a id='dataset'></a>\n",
    "\n",
    "7. Load the data from the `Building_Violations.csv` file into a SparkSession DataFrame. Click in the next code cell and select **Insert to code > Insert SparkSession DataFrame** under the file name.\n",
    "8. Replace `dfData1` with `violations`.\n",
    "9. Replace `dfData1.show(5)` with `violations.printSchema(5)`.\n",
    "10. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>           (4 + 1) / 5]+-------+----------------------------+--------------+--------------+----------------+---------------------+---------------------+------------------+----------------------------+--------------------+------------+-----------------+-----------------+-----------------+-------------------+-----------------+-------------------+-------------+----------------+-----------+-----------+--------------+----+------------+-------------+--------------------+\n",
      "|     ID|VIOLATION LAST MODIFIED DATE|VIOLATION DATE|VIOLATION CODE|VIOLATION STATUS|VIOLATION STATUS DATE|VIOLATION DESCRIPTION|VIOLATION LOCATION|VIOLATION INSPECTOR COMMENTS| VIOLATION ORDINANCE|INSPECTOR ID|INSPECTION NUMBER|INSPECTION STATUS|INSPECTION WAIVED|INSPECTION CATEGORY|DEPARTMENT BUREAU|            ADDRESS|STREET NUMBER|STREET DIRECTION|STREET NAME|STREET TYPE|PROPERTY GROUP| SSA|    LATITUDE|    LONGITUDE|            LOCATION|\n",
      "+-------+----------------------------+--------------+--------------+----------------+---------------------+---------------------+------------------+----------------------------+--------------------+------------+-----------------+-----------------+-----------------+-------------------+-----------------+-------------------+-------------+----------------+-----------+-----------+--------------+----+------------+-------------+--------------------+\n",
      "|1001846|                  05/20/2008|    03/15/2007|        EV0065|        COMPLIED|           04/28/2008|  TEST G & S PASS ELE|         BOTH CARS|                   FULL LOAD|Test governor and...|      541666|          2089609|           PASSED|                N|           PERIODIC|         ELEVATOR| 330 N JEFFERSON ST|          330|               N|  JEFFERSON|         ST|          1901|null|41.887764416| -87.64297689|(41.8877644158757...|\n",
      "|1001847|                  05/20/2008|    03/15/2007|        EV0117|        COMPLIED|           04/28/2008| REP/REPL DEF ALAR...|         BOTH CARS|        PROPERLY PROGRAM ...|Repair or replace...|      541666|          2089609|           PASSED|                N|           PERIODIC|         ELEVATOR| 330 N JEFFERSON ST|          330|               N|  JEFFERSON|         ST|          1901|null|41.887764416| -87.64297689|(41.8877644158757...|\n",
      "|1002009|                  12/07/2011|    04/27/2009|        EV0252|        COMPLIED|           10/25/2011| REMOVE DEBRIS FRO...|              null|                        null|Remove accumulate...|      541348|         10233143|           PASSED|                N|           PERIODIC|         ELEVATOR|5530 N WINTHROP AVE|         5530|               N|   WINTHROP|        AVE|         12102|  26|41.982771573|-87.658138451|(41.9827715727636...|\n",
      "|1002038|                  11/16/2006|    10/02/2006|        199029|            OPEN|                 null|  OPEN TYPE VIOLATION|              null|        RE: BOTH CARS-1. ...|                null|      541348|          1125132|           FAILED|                N|           PERIODIC|         ELEVATOR| 1055 W CATALPA AVE|         1055|               W|    CATALPA|        AVE|         12037|null|41.981733026|-87.657298775|(41.9817330261992...|\n",
      "|1002039|                  11/16/2006|    10/02/2006|        199029|            OPEN|                 null|  OPEN TYPE VIOLATION|              null|        2. INSTALL MISSIN...|                null|      541348|          1598831|           FAILED|                N|           PERIODIC|         ELEVATOR| 1055 W CATALPA AVE|         1055|               W|    CATALPA|        AVE|         12037|null|41.981733026|-87.657298775|(41.9817330261992...|\n",
      "+-------+----------------------------+--------------+--------------+----------------+---------------------+---------------------+------------------+----------------------------+--------------------+------------+-----------------+-----------------+-----------------+-------------------+-----------------+-------------------+-------------+----------------+-----------+-----------+--------------+----+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// @hidden_cell\n",
    "// SparkSession DataFrame goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Modify the data <a id='modify'></a>\n",
    "\n",
    "We’re not going to build a model on all of the data. We need to separate 2006–2016 from 2017. We will use a decade of data to train the model, and we will test the performance of our model on the 2017 data. \n",
    "\n",
    "In the above Schema, `VIOLATION DATE` is string type. This means we need to do some wrangling before we can filter by the dates in an intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create datetime column\n",
    "val dated = violations.withColumn(\"timeStamp\", to_date(unix_timestamp(\n",
    "  $\"VIOLATION DATE\", \"MM/dd/yyyy\"\n",
    ").cast(\"timestamp\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make some more modifications:\n",
    "\n",
    "- Rename all of the columns so that we can reference them more easily later \n",
    "- Remove the space between the names and replace it with an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// sub whitespace for `_`\n",
    "var cleanDf = dated\n",
    "for(col <- dated.columns){\n",
    "    cleanDf = cleanDf.withColumnRenamed(col,col.replaceAll(\"\\\\s\", \"_\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re modeling `INSPECTION_STATUS`, but there are a small number of records where the status has not been resolved into `PASSED` or `FAILED`. Now we will:\n",
    "- Select only those records that meet our criteria with `SQL Transformer`\n",
    "- Change the datatype of ``LATITUDE`` and ``LONGITUDE`` from string to Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "val df = new SQLTransformer().setStatement(\"SELECT * FROM __THIS__ WHERE INSPECTION_STATUS IN ('FAILED', 'PASSED')\").transform(cleanDf)\n",
    "val preppedFrame = df.withColumn(\"LATITUDE\", df(\"LATITUDE\").cast(DoubleType)).\n",
    "                    withColumn(\"LONGITUDE\", df(\"LONGITUDE\").cast(DoubleType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, separate the data by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Filter by date. Train on  year < 2017, test on 2017 data\n",
    "val trainingData2016 = preppedFrame.filter(year($\"timestamp\").leq(lit(2016))) \n",
    "val testingData2017 = preppedFrame.filter(year($\"timestamp\").gt(lit(2016))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `leq` is `less-than-or-equal-to` . `gt` follows the same logic.\n",
    "\n",
    "Now, we’ve represented the DataFrame with a new field, `timeStamp`. We can use this to filter the timestamp data intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    05/16/2017|\n",
      "|    05/16/2017|\n",
      "|    08/27/2017|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Take a peek\n",
    "testingData2017.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|VIOLATION_DATE|\n",
      "+--------------+\n",
      "|    03/15/2007|\n",
      "|    03/15/2007|\n",
      "|    04/27/2009|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// at the training data too\n",
    "trainingData2016.select(\"VIOLATION_DATE\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we choose only a subset of the fields to use for modeling. \n",
    "Many of the other fields have missing values, which is beyond the scope of this notebook. \n",
    "\n",
    "- Specify a subset of the columns\n",
    "- Drop those rows which contain nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keepCols = Array(\"VIOLATION_CODE\", \"VIOLATION_DESCRIPTION\", \n",
    "                   \"INSPECTION_STATUS\", \"INSPECTOR_ID\", \n",
    "                   \"INSPECTION_CATEGORY\", \"DEPARTMENT_BUREAU\", \n",
    "                   \"LATITUDE\", \"LONGITUDE\")\n",
    "val dfTrain = trainingData2016.select(keepCols.head, keepCols.tail: _*).na.drop\n",
    "val dfTest = testingData2017.select(keepCols.head, keepCols.tail: _*).na.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VIOLATION_CODE: string (nullable = true)\n",
      " |-- VIOLATION_DESCRIPTION: string (nullable = true)\n",
      " |-- INSPECTION_STATUS: string (nullable = true)\n",
      " |-- INSPECTOR_ID: string (nullable = true)\n",
      " |-- INSPECTION_CATEGORY: string (nullable = true)\n",
      " |-- DEPARTMENT_BUREAU: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTrain.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## 2. Build a pipeline\n",
    "When you deploy a model to Watson Machine Learning, you need to provide a `Spark Machine Learning Pipeline`, which indicates how to transform raw data into the representation required by our model. \n",
    "Pipelines typically include a series of transformers and terminate with a model or, especially in classification tasks, some transformers which will convert model predictions into string labels.\n",
    "\n",
    "1. [Import transformers](#transformer)\n",
    "2. [Instantiate model object and pipeline](#instantiate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import transformers<a id=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Import transformers to build a pipeline  */ \n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorAssembler}\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the `StringIndexer` to convert strings into a numeric representation for the machine. \n",
    "\n",
    "You can read about many transformations in the <a href=\"https://spark.apache.org/docs/2.0.2/ml-features.html\"  target=\"_blank\" rel=\"noopener noreferrer\">Spark documentation</a>. \n",
    "\n",
    "We assign each transformation a value because we’ll need to reference them later in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, notice that after creating a new instance of `StringIndexer` , we use `setInputCol` and `setOutputCol` . The output column goes into the `VectorAssembler`. We include all of those features we use for modeling in `VectorAssembler`.\n",
    "But what about string data that is not categorical? Sure, we can index all of the `INSPECTOR_ID` data, but does that make sense for the `VIOLATION_DESCRIPTION`, where almost every field is unique? \n",
    "<br>\n",
    "\n",
    "For text data like this, Scala and Spark provide other handy transformations. For `RegexTokenizer` and `HashingTF` the  idea is simple. The tokenizer takes the text and breaks it into individual words, called `tokens`. Map the tokens contained in each violation description to their frequencies. This allows us to accept unseen data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>         (5 + 1) / 6]"
     ]
    }
   ],
   "source": [
    "// Label colum\n",
    "val labelCol = new StringIndexer().setInputCol(\"INSPECTION_STATUS\").setOutputCol(\"STATUS_LABEL\").fit(df)\n",
    "\n",
    "// Feature cols with String Indexer => Vector Assembler //\n",
    "\n",
    "//* VIOLATION CODE * //\n",
    "val interCodeCol = new StringIndexer().setInputCol(\"VIOLATION_CODE\").setOutputCol(\"CODE_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTOR ID * //\n",
    "val interSpector = new StringIndexer().setInputCol(\"INSPECTOR_ID\").setOutputCol(\"INSP_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* INSPECTION CATEGORY * //\n",
    "val interCatSpector = new StringIndexer().setInputCol(\"INSPECTION_CATEGORY\").setOutputCol(\"INCAT_X\").setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "//* DEPARTMENT BUREAU * //\n",
    "val interBureau = new StringIndexer().setInputCol(\"DEPARTMENT_BUREAU\").setHandleInvalid(\"skip\").setOutputCol(\"BUR_X\")\n",
    "\n",
    "\n",
    "//** DEALING WITH TEXT **//\n",
    "val regexTokenizer = new RegexTokenizer().setInputCol(\"VIOLATION_DESCRIPTION\").setOutputCol(\"WORD_X\").setPattern(\"\\\\W\")\n",
    "val hashingTF = new HashingTF().setInputCol(\"WORD_X\").setOutputCol(\"DESCRIPTION\").setNumFeatures(150) // experiment with numFeatures + regularization params\n",
    "\n",
    "// LAT AND LONG ARE NUMERIC //\n",
    "\n",
    "\n",
    "//** VECTOR ASSEMBLER **//\n",
    "\n",
    "val vecAssembler = new VectorAssembler().setInputCols(Array(\"BUR_X\", \"INCAT_X\", \"CODE_X\", \"INSP_X\", \"DESCRIPTION\", \"LATITUDE\", \"LONGITUDE\")).setOutputCol(\"FEATURES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instantiate the model object and pipeline<a id=\"instantiate\"></a>\n",
    "Time to instantiate a new untrained model object and pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "//** Logistic Regression **//\n",
    "val logitModel = new LogisticRegression().setLabelCol(\"STATUS_LABEL\").setFeaturesCol(\"FEATURES\").setRegParam(0.1)\n",
    "\n",
    "\n",
    "//** Convert index prediction back to string **//\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"PREDICTED_LABEL\").setLabels(labelCol.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Logitic Regression Pipeline */ \n",
    "val logisticPipe = new Pipeline().setStages(\n",
    "                                    Array(\n",
    "                                        labelCol, \n",
    "                                        interCodeCol, \n",
    "                                        interSpector, \n",
    "                                        interCatSpector,\n",
    "                                        interBureau,\n",
    "                                        regexTokenizer, hashingTF,\n",
    "                                        vecAssembler,\n",
    "                                        logitModel                                                                  \n",
    "                                    )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 3. Train the models\n",
    "Call `.fit()` on the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 16:================================================>         (5 + 1) / 6]"
     ]
    }
   ],
   "source": [
    "val trainedLogit = logisticPipe.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions and get metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "// predict\n",
    "val predictionsLogisitc = trainedLogit.transform(dfTest)\n",
    "\n",
    "\n",
    "// Prepare for metrics\n",
    "val predictionAndLabels = predictionsLogisitc.select(\"STATUS_LABEL\", \"prediction\").rdd.map(row => \n",
    "            (row.getAs[Double](\"prediction\"), row.getAs[Double](\"STATUS_LABEL\")))\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a new object `metrics`, which contains a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 278:======================================>                  (4 + 2) / 6]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.664035034778375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// AUC\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An area of .5 under the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\"  target=\"_blank\" rel=\"noopener noreferrer\">ROC Curve</a> indicates that the model performs as well as random guessing, so we’ve beaten that enough to continue for purposes of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 4. Save the model\n",
    "You’ll need an instance of [Watson Machine Learning](https://developer.ibm.com/clouddataservices/docs/ibm-watson-machine-learning/) to save your model. \n",
    "\n",
    "You can create a new instance directly from within DSX, but you’ll need to log in to IBM Cloud for your credentials. IBM Cloud offers many powerful services and several are available at little to no cost, including Watson Machine Learning (WML). With WML, you have a repository to store and deploy your models. Consult <a href=\"https://developer.ibm.com/clouddataservices/docs/ibm-watson-machine-learning/get-started/\"  target=\"_blank\" rel=\"noopener noreferrer\">IBM Developer resources</a> to help you get started if you haven't already. You can also check the companion <a href=\"https://medium.com/p/91b580450c5b/edit\"  target=\"_blank\" rel=\"noopener noreferrer\">blog</a> for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the <a href=\"hhttps://watson-ml-libs.mybluemix.net/repository-scalaV3/#com.ibm.analytics.ngp.repository_v3.package\"  target=\"_blank\" rel=\"noopener noreferrer\">IBM Scala Repository API Client for Watson Machine Learning</a> and other helpful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.ibm.analytics.ngp.repository._\n",
    "\n",
    "// Helper libraries\n",
    "\n",
    "import scalaj.http.{Http, HttpOptions}\n",
    "import scala.util.{Success, Failure}\n",
    "import java.util.Base64\n",
    "import java.nio.charset.StandardCharsets\n",
    "import play.api.libs.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fetch your credentials. The WML credentials enable you to communicate with your repository via the internet. \n",
    "Watch the <a href=\"https://developer.ibm.com/clouddataservices/docs/ibm-watson-machine-learning/get-started/\"  target=\"_blank\" rel=\"noopener noreferrer\">Getting started</a> video to find out how to create your credentials, then copy the credentials to the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//val service_path = \"service_path\"\n",
    "//val access_key = \"access_key\"\n",
    "//val username = \"username\"\n",
    "//val password = \"password\"\n",
    "//val instance_id = \"instance_id\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make a connection and authorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Success(())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Authorize\n",
    "val client = MLRepositoryClient(service_path)\n",
    "client.authorize(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `MLRepositoryArtifact` to create a model artifact for the repository. We must pass:\n",
    "- A Spark ML pipeline: `trainedLogit`\n",
    "- The training data used: `dfTrain`\n",
    "- A name for the model:`VIOLATIONS_SCALA211_SPARK20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// model artifact\n",
    "val model_artifact = MLRepositoryArtifact(trainedLogit, dfTrain, \"VIOLATIONS_SCALA211_SPARK20\")\n",
    "val saved = client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saved` is the model artifact. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelType: Some(sparkml-model-2.0)\n",
      "trainingDataSchema: Some({\"type\":\"struct\",\"fields\":[{\"name\":\"VIOLATION_CODE\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"VIOLATION_DESCRIPTION\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_STATUS\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTOR_ID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INSPECTION_CATEGORY\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DEPARTMENT_BUREAU\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LATITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"LONGITUDE\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]})\n",
      "creationTime: Some(2018-01-31T15:54:10.360Z)\n",
      "modelVersionHref: Some(https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/084f19c9-39d3-4572-ace2-bd48bae6ceb4/versions/72bb489f-eadb-47d4-a758-90b135665d82)\n",
      "label: Some(INSPECTION_STATUS)\n",
      "runtime: Some(spark-2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"modelType: \" + saved.get.meta.prop(\"modelType\"))\n",
    "println(\"trainingDataSchema: \" + saved.get.meta.prop(\"trainingDataSchema\"))\n",
    "println(\"creationTime: \" + saved.get.meta.prop(\"creationTime\"))\n",
    "println(\"modelVersionHref: \" + saved.get.meta.prop(\"modelVersionHref\"))\n",
    "println(\"label: \" + saved.get.meta.prop(\"label\"))\n",
    "println(\"runtime: \"+ saved.get.meta.prop(\"runtime\"))\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "## 5. Deploy the model from within your DSX project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve saved the model, we can deploy and create an API endpoint to score records. This part of the notebook shows you how to do this from the UI with Cloud. \n",
    "\n",
    "If you're following along in the [blog](https://medium.com/p/91b580450c5b/edit), this is where we'll deploy through the UI. \n",
    "\n",
    "1. Go to **Assets** in your Project, then find your model under **Models**. We named ours `VIOLATIONS_SCALA211_SPARK20`.\n",
    "2. Select **Deployment > Add Deployment**.\n",
    "3. Enter a name and description for the deployment, click **Save**.\n",
    "4. Select the model, click **Test** to test the API. \n",
    "\n",
    "See [here](https://dataplatform.ibm.com/docs/content/analyze-data/ml-deploy-project.html?context=analytics) for more information about how to deploy a model within a project.\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next steps<a id=\"summary\"></a>\n",
    "That's awesome! We built a model with Scala and Spark on DSX Cloud. \n",
    "\n",
    "For the next step, we will expose the model as an API endpoint and score new records from R Studio using DSX Local. \n",
    "\n",
    "How to do this is described <!--in the R notebook *Score new data with R4WML and R*. \n",
    "You can also follow along --> in the <a href=\"https://medium.com/@adammassachi/dsx-hybrid-mode-91b580450c5b\"  target=\"_blank\" rel=\"noopener noreferrer\">blog</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "City of Chicago (2017). Building Violations <a href=https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr target=\"_blank\" rel=\"noopener noreferrer\">https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr</a>  Chicago, IL: Chicago City Data Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "**Adam Massachi** is a Data Scientist with the Data Science Experience and Watson Data Platform teams at IBM. Before IBM, he worked on political campaigns, building and managing large volunteer operations and organizing campaign finance initiatives. Say hello [@adammassach](https://twitter.com/adammassach?lang=en)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.0",
   "language": "scala",
   "name": "scala-spark20"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
